# Pre-Mortem: Maintenance Risks

*It's 2027. The project has become a maintenance nightmare. Here's what went wrong and what could have prevented it.*

---

## 1. Hardcoded temp paths create race conditions and silent corruption

`scrub_voices.py` writes to hardcoded `temp_audio.wav` and `output/` in the current working directory. If two containers somehow share a volume, or if the function is ever called concurrently (e.g., making `process_all` parallel), files silently overwrite each other.

**Prevention:** Use `tempfile.mkdtemp()` / `tempfile.NamedTemporaryFile()` for all intermediate artifacts. Tie temp directories to the input filename or a UUID.

---

## 2. `sys.exit()` as error handling makes the code non-composable

Nearly every function in the codebase calls `sys.exit(1)` on failure — `_api()`, `_require_env()`, `validate_file()`, `crop_video_9_16()`, etc. This means no function can be safely called from another Python program, a test harness, or a web endpoint without killing the entire process.

**Prevention:** Raise typed exceptions (`VideoProcessingError`, `UploadError`) and let the CLI `__main__` blocks be the only place that catches and exits. This also enables retry logic and proper error reporting.

---

## 3. No tests — regression risk grows with every change

There are zero test files. Every change to cropping logic, suffix conventions, or API interactions is deployed on faith. A single typo in `KNOWN_SUFFIXES` could reprocess every video in the library.

**Prevention:** Add a `tests/` directory from day one. Unit-test pure logic (suffix matching, URL building, filename generation). Use `unittest.mock` to stub moviepy/spleeter/API calls for integration-level tests. Run tests in CI.

---

## 4. Suffix-based state machine is fragile and implicit

The system tracks processing state entirely through filename suffixes: `_cropped_9_16`, `_novocals`, `_processing`, `_final`. The list in `process_all_videos.py` must be kept perfectly in sync with the suffixes generated by `crop_video.py`, `scrub_voices.py`, and `process_one_video.py`. Adding a new pipeline step (e.g., normalization, watermarking) means updating multiple files or breaking idempotency detection.

**Prevention:** Use a manifest file (JSON/SQLite) to track processing state per video, or adopt a single status enum rather than scattering conventions across filenames.

---

## 5. No resource cleanup on moviepy/spleeter failures

`crop_video.py` opens a `VideoFileClip` but only calls `.close()` after a successful write. If `write_videofile()` throws, the file handle leaks. Same pattern in `scrub_voices.py` — `video`, `new_audio`, `new_video` are in a flat sequence with cleanup at the end, not in `try/finally` or context managers.

**Prevention:** Use `with` statements or `try/finally` for all moviepy clips. `VideoFileClip` supports `__enter__`/`__exit__`.

---

## 6. Pinned Graph API version will silently rot

`instagram_upload.py` hardcodes `v21.0`. Meta deprecates Graph API versions roughly every 2 years. When v21.0 is removed, uploads will break with no warning.

**Prevention:** Make the API version an environment variable with a default. Add a startup check that logs the current version and warns if it's been deprecated (Meta returns deprecation headers).

---

## 7. No retry logic for network operations

The Instagram upload flow (create → poll → publish) has zero retries. A single transient 500 from Meta or a brief ngrok tunnel hiccup kills the entire upload with `sys.exit(1)`. The polling loop in `wait_for_container()` treats any `HTTPError` as fatal.

**Prevention:** Add exponential backoff with 2–3 retries for transient HTTP errors (429, 500, 502, 503). Distinguish transient from permanent failures.

---

## 8. Inconsistent indentation (tabs vs. spaces)

`crop_video.py` and `scrub_voices.py` use tabs; `process_one_video.py`, `process_all_videos.py`, and `instagram_upload.py` use 4-space indentation. Mixing these causes `TabError` if someone pastes code between files.

**Prevention:** Add a `.editorconfig` or `pyproject.toml` with formatting rules. Run a linter (`ruff`, `flake8`) in CI or as a pre-commit hook.

---

## 9. Spleeter model download happens at runtime, inside Docker

Spleeter downloads its ~100 MB pre-trained model on first use. Because the Docker image doesn't cache it, every `docker compose run --rm scrub_voices` re-downloads the model if the container is recreated. This is slow, fragile (relies on GitHub/remote model host), and fails offline.

**Prevention:** Pre-download the model during `docker build` (`RUN python -c "from spleeter.separator import Separator; Separator('spleeter:2stems')"`) or mount a persistent volume for the model cache.

---

## 10. Flat `src/` with cross-imports but no package structure

All six scripts live in a flat `src/` directory with `PYTHONPATH` manipulation. `process_one_video.py` does `from crop_video import ...` and `from scrub_voices import ...` — bare imports that only work because of the Docker `ENV PYTHONPATH`. Running any script outside Docker (for debugging, testing) requires manually setting the path.

**Prevention:** Make `src/` a proper Python package with `__init__.py`, or better, use a `pyproject.toml` with an installable package. This enables `pip install -e .` for local dev and proper import resolution everywhere.

---

## 11. Webhook server is a stub with no security hardening

`webhook_server.py` accepts arbitrary POST payloads with a `TODO` comment and returns 200. If the ngrok tunnel is public, anyone can POST to `/webhook/instagram`. The `VERIFY_TOKEN` check only applies to the GET verification handshake, not to incoming POST events (Meta signs payloads with `X-Hub-Signature-256`, but this is never validated).

**Prevention:** Validate the `X-Hub-Signature-256` header on every POST using the app secret. Rate-limit the endpoint. Actually implement the event handler or remove the service.

---

## 12. No logging, no observability

Processing scripts use only `print()`. There are no log levels, no timestamps, no structured output. When a batch of 50 videos fails at video #37, the only trace is scrolling through terminal output. There's no way to search logs, alert on failures, or correlate events.

**Prevention:** Use Python `logging` with a consistent format (timestamp, level, module). For batch runs, write a summary JSON/CSV alongside console output.

---

## Summary

| # | Risk | Severity | Effort to Fix |
|---|---|---|---|
| 1 | Hardcoded temp paths | High | Low |
| 2 | `sys.exit()` everywhere | High | Medium |
| 3 | No tests | High | Medium |
| 4 | Fragile suffix state machine | Medium | Medium |
| 5 | No resource cleanup | Medium | Low |
| 6 | Pinned API version | Medium | Low |
| 7 | No retry logic | Medium | Low |
| 8 | Mixed tabs/spaces | Low | Low |
| 9 | Runtime model download | Medium | Low |
| 10 | No package structure | Medium | Low |
| 11 | Webhook security | Medium | Medium |
| 12 | No logging/observability | Medium | Medium |

The highest-leverage fixes are raising exceptions instead of exiting, adding basic tests, and using temp directories — each is low-to-medium effort and would prevent the most painful maintenance scenarios.
